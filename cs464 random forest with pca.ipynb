{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a playlist of more relevant recommended tracks of a playlist\n",
    "\n",
    "# install Spotipy first\n",
    "#!pip install Spotipy\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "from spotipy import oauth2\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read cache at: .cache-hpa6zlnetl6hib5bnp1b5laul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the URL you were redirected to: https://www.google.com.tr/?code=AQDnGhCb0JZelhAHeZY2ZenrmKWHLnw7V4CEk94WWbL7uqD-z87xkugPrk3frKRyTQYjuFCADAwS9LW2iwfCoQjNYHjj4wqDNUbQPPv0XEvwx6sW00VzOs84ifG7kycQL2nr-AapP-JWgbR_RWc9DH33ISoQT3pTGo9misKVJxbOg2JiqvcWbW0QWbmoLlop7Powi8vYTnUewGzwX97WCumP5CDxf_e7pFOe18YCBzfihZqyJ3WBrBayYPC_jSv-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read cache at: .cache-hpa6zlnetl6hib5bnp1b5laul\n"
     ]
    }
   ],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import spotipy.util as util\n",
    "\n",
    "\n",
    "cid ='a611ab78dcf5445ea1d91bf605134cbd' # Client ID; copy this from your app created on beta.developer.spotify.com\n",
    "secret = 'c196412417624657a55c5f71c44fd3b9' # Client Secret; copy this from your app\n",
    "username = 'hpa6zlnetl6hib5bnp1b5laul' # Your Spotify username\n",
    "\n",
    "#for avaliable scopes see https://developer.spotify.com/web-api/using-scopes/\n",
    "scope = 'user-library-read playlist-modify-public playlist-read-private'\n",
    "\n",
    "redirect_uri='https://www.google.com.tr/'\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret) \n",
    "\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "token = util.prompt_for_user_token(username, scope, cid, secret, redirect_uri)\n",
    "\n",
    "if token:\n",
    "    sp = spotipy.Spotify(auth=token)\n",
    "else:\n",
    "    print(\"Can't get token for\", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ce70771c7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtoken_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"data.csv\"\n",
    "with open(dataset_path, 'r', encoding=\"utf8\") as token_file:\n",
    "    lines = csv.reader(token_file)\n",
    "    dataset = list(lines)\n",
    "    data = np.array(dataset)\n",
    "data = data[1:, :]\n",
    "print(data.shape)\n",
    "song_valence = data[:, 17]\n",
    "song_acousticness = data[:, 0]\n",
    "song_artists = data[:, 1]\n",
    "song_danceability = data[:, 2]\n",
    "\n",
    "song_duration_ms = data[:, 3]\n",
    "song_energy = data[:, 4]\n",
    "song_id = data[:, 6]\n",
    "song_popularity = data[:, 13]\n",
    "\n",
    "song_instrumentalness = data[:, 7]\n",
    "song_key = data[:, 8]\n",
    "song_liveness = data[:, 9]\n",
    "song_loudness = data[:, 10]\n",
    "\n",
    "song_mode = data[:, 11]\n",
    "song_name = data[:, 12]\n",
    "song_speechiness = data[:, 15]\n",
    "song_tempo = data[:, 16]\n",
    "\n",
    "song_info = {'name': [], 'id': [], 'artists': []}\n",
    "features_dict = {'valence': [], 'popularity': [], 'acousticness': [], 'danceability': [], 'duration_ms': [], 'energy': [], 'instrumentalness': [], 'key': [], 'liveness': [], 'loudness': [], 'mode': [], 'speechiness': [], 'tempo': []}\n",
    "for i in range(0, data.shape[0]):\n",
    "    song_info['name'].append(song_name[i])\n",
    "    song_info['id'].append(song_id[i])\n",
    "    song_info['artists'].append(song_artists[i])\n",
    "    features_dict['valence'].append(song_valence[i])\n",
    "    features_dict['acousticness'].append(song_acousticness[i])\n",
    "    features_dict['danceability'].append(song_danceability[i])\n",
    "    features_dict['duration_ms'].append(song_duration_ms[i])\n",
    "    features_dict['popularity'].append(song_popularity[i])\n",
    "    features_dict['energy'].append(song_energy[i])\n",
    "    features_dict['instrumentalness'].append(song_instrumentalness[i])\n",
    "    features_dict['key'].append(song_key[i])\n",
    "    features_dict['liveness'].append(song_liveness[i])\n",
    "    features_dict['loudness'].append(song_loudness[i])\n",
    "    features_dict['mode'].append(song_mode[i])\n",
    "    features_dict['speechiness'].append(song_speechiness[i])\n",
    "    features_dict['tempo'].append(song_tempo[i])\n",
    "playlist_df = pd.DataFrame(features_dict, index = song_info['name'])\n",
    "playlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of your playlist including tracks' names and audio features\n",
    "sourcePlaylistID = '5TplGaik7PSSBqH6eRRE8E'\n",
    "sourcePlaylist = sp.user_playlist(username, sourcePlaylistID);\n",
    "tracks = sourcePlaylist[\"tracks\"];\n",
    "songs = tracks[\"items\"];\n",
    "\n",
    "track_ids = []\n",
    "track_names = []\n",
    "\n",
    "for i in range(0, len(songs)):\n",
    "    if songs[i]['track']['id'] != None: # Removes the local tracks in your playlist if there is any\n",
    "        track_ids.append(songs[i]['track']['id'])\n",
    "        track_names.append(songs[i]['track']['name'])\n",
    "\n",
    "features = []\n",
    "for i in range(0,len(track_ids)):\n",
    "    audio_features = sp.audio_features(track_ids[i])\n",
    "    tr = sp.track(track_ids[i])['popularity']\n",
    "    for track in audio_features:\n",
    "        print(track)\n",
    "        features.append(track)\n",
    "#print(features)\n",
    "playlist_df = pd.DataFrame(features, index = track_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#playlist_df=playlist_df[[\"id\", \"acousticness\", \"danceability\", \"duration_ms\", \n",
    "#                         \"energy\", \"instrumentalness\",  \"key\", \"liveness\",\n",
    "#                         \"loudness\", \"mode\", \"speechiness\", \"tempo\", \"valence\"]]\n",
    "#playlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#v=TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 6), max_features=10000)\n",
    "#X_names_sparse = v.fit_transform(track_names)\n",
    "#X_names_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the below info for each playlist manually to get better results\n",
    "# Give ratings to your tracks with respect to their playlist relevances\n",
    "# Rate them from 1-10, give higher ratings to those tracks which you think best chracterizes your playlist\n",
    "# If you order your playlist by relevance while creating it, this step will become easier\n",
    "# So now, we will deal with a classification task\n",
    "\n",
    "features_array = np.zeros((data.shape[0]+1, 11))\n",
    "\n",
    "input_ = np.zeros((11))\n",
    "input_[0] = playlist_df['valence'][0]\n",
    "input_[1] = playlist_df['acousticness'][0]\n",
    "input_[2] = playlist_df['danceability'][0]\n",
    "input_[3] = playlist_df['popularity'][0]\n",
    "input_[4] = playlist_df['energy'][0]\n",
    "input_[5] = playlist_df['instrumentalness'][0]\n",
    "input_[6] = playlist_df['liveness'][0]\n",
    "input_[7] = playlist_df['loudness'][0]\n",
    "input_[8] = playlist_df['mode'][0]\n",
    "input_[9] = playlist_df['speechiness'][0]\n",
    "input_[10] = playlist_df['tempo'][0]\n",
    "\n",
    "features_array[data.shape[0], :] = input_\n",
    "\n",
    "features_array[:data.shape[0],0] = np.array(features_dict['valence']).T\n",
    "features_array[:data.shape[0],1] = np.array(features_dict['acousticness']).T\n",
    "features_array[:data.shape[0],2] = np.array(features_dict['danceability']).T\n",
    "features_array[:data.shape[0],3] = np.array(features_dict['popularity']).T\n",
    "features_array[:data.shape[0],4] = np.array(features_dict['energy']).T\n",
    "features_array[:data.shape[0],5] = np.array(features_dict['instrumentalness']).T\n",
    "features_array[:data.shape[0],6] = np.array(features_dict['liveness']).T\n",
    "features_array[:data.shape[0],7] = np.array(features_dict['loudness']).T\n",
    "features_array[:data.shape[0],8] = np.array(features_dict['mode']).T\n",
    "features_array[:data.shape[0],9] = np.array(features_dict['speechiness']).T\n",
    "features_array[:data.shape[0],10] = np.array(features_dict['tempo']).T\n",
    "\n",
    "features_array = features_array.astype('float64')\n",
    "\n",
    "model = NearestNeighbors(n_neighbors = 1000, algorithm = 'ball_tree')\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scalar.fit(features_array)\n",
    "features_array = scalar.transform(features_array)\n",
    "input_2 = features_array[data.shape[0], :]\n",
    "features_array = features_array[:data.shape[0], :]\n",
    "model.fit(features_array)\n",
    "\n",
    "#scalar2 = StandardScaler()\n",
    "#scalar2.fit([input_])\n",
    "#input_2 = scalar2.transform([input_])\n",
    "\n",
    "\n",
    "print(features_array.shape)\n",
    "\n",
    "distances, indices = model.kneighbors([input_2])\n",
    "recorded_indices = indices\n",
    "print(distances[0, :10])\n",
    "print(distances[0, 990:])\n",
    "\n",
    "closest_1000_point = {'valence': [], 'popularity': [], 'mode': [], 'acousticness': [], 'danceability': [], 'energy': [], 'instrumentalness': [], 'liveness': [], 'loudness': [], 'speechiness': [], 'tempo': [], 'ratings': []}\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    closest_1000_point['valence'].append(features_array[indices[0, i], 0])\n",
    "    closest_1000_point['acousticness'].append(features_array[indices[0, i], 1])\n",
    "    closest_1000_point['danceability'].append(features_array[indices[0, i], 2])\n",
    "    closest_1000_point['popularity'].append(features_array[indices[0, i], 3])\n",
    "    closest_1000_point['energy'].append(features_array[indices[0, i], 4])\n",
    "    closest_1000_point['instrumentalness'].append(features_array[indices[0, i], 5])\n",
    "    closest_1000_point['liveness'].append(features_array[indices[0, i], 6])\n",
    "    closest_1000_point['loudness'].append(features_array[indices[0, i], 7])\n",
    "    closest_1000_point['mode'].append(features_array[indices[0, i], 8])\n",
    "    closest_1000_point['speechiness'].append(features_array[indices[0, i], 9])\n",
    "    closest_1000_point['tempo'].append(features_array[indices[0, i], 10])\n",
    "    \n",
    "    rem = i // 100\n",
    "    closest_1000_point['ratings'].append(10-rem)\n",
    "#playlist_df['ratings']=[10, 9, 9, 10, 8, 6, 8, 6, 4, 3, 6, 9]\n",
    "#playlist_df.head()\n",
    "playlist_df = pd.DataFrame(closest_1000_point)\n",
    "playlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importances\n",
    "from sklearn.ensemble.forest import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "X_train = playlist_df.drop(['ratings'], axis=1)\n",
    "y_train = playlist_df['ratings']\n",
    "forest = RandomForestClassifier(random_state=42, max_depth=5, max_features=11) # Set by GridSearchCV below\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature rankings\n",
    "print(\"Feature ranking:\")\n",
    "  \n",
    "for f in range(len(importances)):\n",
    "    print(\"%d. %s %f \" % (f + 1, \n",
    "            X_train.columns[indices[f]], \n",
    "            importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pca to the scaled train set first\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style='white')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X_train)\n",
    "print(X_scaled.shape)\n",
    "pca = decomposition.PCA().fit(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 12)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(8, c='b') # Tune this so that you obtain at least a 95% total variance explained\n",
    "plt.axhline(0.95, c='r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your dataset to the optimal pca\n",
    "pca1 = decomposition.PCA(n_components=8)\n",
    "X_pca = pca1.fit_transform(X_scaled)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to check the results of TSNE also\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#tsne = TSNE(random_state=17)\n",
    "#X_tsne = tsne.fit_transform(X_scaled)\n",
    "#print(X_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "#X_train_last = csr_matrix(hstack([X_pca, X_names_sparse])) # Check with X_tsne + X_names_sparse also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize a stratified split for the validation process\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    # Decision Trees First\n",
    "    X_train_cv = np.array(X_train)[train_index, :]\n",
    "    X_test_cv = np.array(X_train)[test_index, :]\n",
    "    y_train_cv = np.array(y_train)[train_index]\n",
    "    y_test_cv = np.array(y_train)[test_index]\n",
    "    #tree = DecisionTreeClassifier(criterion = \"entropy\", splitter = \"random\")\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree_params = {'max_depth': range(1,11), 'max_features': range(1,11)}\n",
    "\n",
    "    tree_grid = GridSearchCV(tree, tree_params, n_jobs=-1, verbose=True)\n",
    "\n",
    "    tree_grid.fit(X_train_cv, y_train_cv)\n",
    "    print(tree_grid.best_estimator_)\n",
    "    print(tree_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests second\n",
    "\n",
    "parameters = {'max_features': range(1,11), 'min_samples_leaf': range(1,11), 'max_depth': range(1, 11)}\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                             n_jobs=-1)\n",
    "best_score = 0\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    # Decision Trees First\n",
    "    X_train_cv = np.array(X_train)[train_index, :]\n",
    "    X_test_cv = np.array(X_train)[test_index, :]\n",
    "    y_train_cv = np.array(y_train)[train_index]\n",
    "    y_test_cv = np.array(y_train)[test_index]\n",
    "    gcv1 = GridSearchCV(rfc, parameters, n_jobs=-1, verbose=1)\n",
    "    gcv1.fit(X_train_cv, y_train_cv)\n",
    "    print(gcv1.best_estimator_)\n",
    "    print(gcv1.best_score_)\n",
    "    score = gcv1.best_score_\n",
    "    if score > best_score:\n",
    "        recorded_train_indexes = train_index\n",
    "        recorded_test_indexes = test_index\n",
    "        recorded_best_parameters = gcv1.best_estimator_\n",
    "        \n",
    "print(recorded_best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN third\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# knn_params = {'n_neighbors': range(1, 2)}\n",
    "# knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "# knn_grid = GridSearchCV(knn, knn_params, cv=skf, n_jobs=-1, verbose=True)\n",
    "# knn_grid.fit(X_train_last, y_train)\n",
    "# knn_grid.best_params_, knn_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't expect high cross-val scores! Our dataset is way too small...\n",
    "For my case, PCA + Decision Trees seems to perform better than worst.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build your test set;\n",
    "# Generate a new dataframe for recommended tracks\n",
    "# Set recommendation limit as half the Playlist Length per track, you may change this as you like\n",
    "# Check documentation for  recommendations; https://beta.developer.spotify.com/documentation/web-api/reference/browse/get-recommendations/\n",
    "\n",
    "#rec_tracks = []\n",
    "#for i in playlist_df['id'].values.tolist():\n",
    "#    rec_tracks += sp.recommendations(seed_tracks=[i], limit=3)['tracks'];\n",
    "#\n",
    "#rec_track_ids = []\n",
    "#rec_track_names = []\n",
    "#for i in rec_tracks:\n",
    "#    rec_track_ids.append(i['id'])\n",
    "#    rec_track_names.append(i['name'])\n",
    "#\n",
    "#rec_features = []\n",
    "#for i in range(0,len(rec_track_ids)): \n",
    "#    rec_audio_features = sp.audio_features(rec_track_ids[i])\n",
    "#    for track in rec_audio_features:\n",
    "#        rec_features.append(track)\n",
    "#        \n",
    "#rec_playlist_df = pd.DataFrame(rec_features, index = rec_track_ids)\n",
    "#rec_playlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_names = v.transform(rec_track_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_playlist_df=rec_playlist_df[[\"acousticness\", \"danceability\", \"duration_ms\", \n",
    "                         \"energy\", \"instrumentalness\",  \"key\", \"liveness\",\n",
    "                         \"loudness\", \"mode\", \"speechiness\", \"tempo\", \"valence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "X_train_set = np.array(X_train)[recorded_train_indexes, :]\n",
    "X_test_set = np.array(X_train)[recorded_test_indexes, :]\n",
    "y_train_set = np.array(y_train)[recorded_train_indexes]\n",
    "y_test_set = np.array(y_train)[recorded_test_indexes]\n",
    "gcv1.best_estimator_ = recorded_best_parameters\n",
    "gcv1.best_estimator_.fit(X_train_set, y_train_set)\n",
    "#rec_playlist_df_scaled = StandardScaler().fit_transform(rec_playlist_df)\n",
    "#rec_playlist_df_pca = pca1.transform(rec_playlist_df_scaled)\n",
    "#X_test_last = csr_matrix(hstack([rec_playlist_df_pca, X_test_names]))\n",
    "y_pred_class = gcv1.best_estimator_.predict(X_test_set)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_playlist_ratings = y_pred_class\n",
    "#rec_playlist_df = rec_playlist_df.sort_values('ratings', ascending = False)\n",
    "#rec_playlist_df = rec_playlist_df.reset_index()\n",
    "# Pick the top ranking tracks to add your new playlist 9, 10 will work\n",
    "ten_indexes = []\n",
    "nine_indexes = []\n",
    "for i in range(0, len(y_pred_class)):\n",
    "    if y_pred_class[i] == 10:\n",
    "        ten_indexes.append(i)\n",
    "#    if y_pred_class[i] == 9:\n",
    "#        nine_indexes.append(i)\n",
    "#recs_to_add = rec_playlist_df[rec_playlist_df['ratings']>=9]['index'].values.tolist()\n",
    "\n",
    "recomended_song_indexes = []\n",
    "for i in ten_indexes:\n",
    "    recomended_song_indexes.append(recorded_indices[0, test_index[i]])\n",
    "#for i in nine_indexes:\n",
    "#    recomended_song_indexes.append(recorded_indices[0, test_index[i]])\n",
    "print(recomended_song_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No ratings of 9 or 10 this case try adding 8's only\n",
    "#recs_to_add = rec_playlist_df[rec_playlist_df['ratings']==8]['index'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what is about to happen :)\n",
    "for i in recomended_song_indexes:\n",
    "    print(song_info['id'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rec_array = np.array(recs_to_add[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new playlist for tracks to add - you may also add these tracks to your source playlist and proceed\n",
    "playlist_recs = sp.user_playlist_create(username, \n",
    "                                        name='CS464 - {}'.format(sourcePlaylist['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tracks to the new playlist\n",
    "tracks = {}\n",
    "for i in recomended_song_indexes:\n",
    "    tracks = {song_info['id'][i]}\n",
    "    print(tracks)\n",
    "    sp.playlist_add_items(playlist_recs['id'], tracks, position=None)\n",
    "#     sp.user_playlist_add_tracks(username, playlist_recs['id'], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This performed really well for my case, and I am pretty satisfied with the resulting playlist -more than the default recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
